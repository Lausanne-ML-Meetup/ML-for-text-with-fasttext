{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fastext for text classification\n",
    "\n",
    "Source paper: https://arxiv.org/abs/1607.01759\n",
    "\n",
    "I am using the `yelp_review_full` dataset for my experiments, you can find all the sentiment corpora used in the paper [here](https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M).\n",
    "    \n",
    "## Bag of tricks and preprocessing\n",
    "\n",
    "Just a cleaned python version of the fasttext preprocesing code, see `dictionary.cc`. \n",
    "\n",
    "\n",
    "### Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Token():\n",
    "    \"\"\"A simple class to represent the tokens.\"\"\"\n",
    "    def __init__(self, word, count, is_label):\n",
    "        self.word = word\n",
    "        self.count = count\n",
    "        self.is_label = is_label\n",
    "    def __str__(self):\n",
    "        return '('+self.word+', '+str(self.count)+')'\n",
    "    def __repr__(self):\n",
    "        return '('+self.word+', '+str(self.count)+')'\n",
    "    \n",
    "def cmp_to_key(mycmp):\n",
    "    \"\"\"Convert a cmp= function into a key= function\"\"\"\n",
    "    class K:\n",
    "        def __init__(self, obj, *args):\n",
    "            self.obj = obj\n",
    "        def __lt__(self, other):\n",
    "            return mycmp(self.obj, other.obj) < 0\n",
    "        def __gt__(self, other):\n",
    "            return mycmp(self.obj, other.obj) > 0\n",
    "        def __eq__(self, other):\n",
    "            return mycmp(self.obj, other.obj) == 0\n",
    "        def __le__(self, other):\n",
    "            return mycmp(self.obj, other.obj) <= 0\n",
    "        def __ge__(self, other):\n",
    "            return mycmp(self.obj, other.obj) >= 0\n",
    "        def __ne__(self, other):\n",
    "            return mycmp(self.obj, other.obj) != 0\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script to normalize the dataset, extracted from `classification-results.sh`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset /media/mat/ssdBackupMat/Datasets/sentiment/yelp_review_full_csv ... \n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "myshuf() {\n",
    "  perl -MList::Util=shuffle -e 'print shuffle(<>);' \"$@\";\n",
    "}\n",
    "\n",
    "normalize_text() {\n",
    "  tr '[:upper:]' '[:lower:]' | sed -e 's/^/__label__/g' | \\\n",
    "    sed -e \"s/'/ ' /g\" -e 's/\"//g' -e 's/\\./ \\. /g' -e 's/<br \\/>/ /g' \\\n",
    "        -e 's/,/ , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\\!/ \\! /g' \\\n",
    "        -e 's/\\?/ \\? /g' -e 's/\\;/ /g' -e 's/\\:/ /g' | tr -s \" \" | myshuf\n",
    "}\n",
    "\n",
    "DATADIR=/media/mat/ssdBackupMat/Datasets/sentiment/yelp_review_full_csv\n",
    "\n",
    "echo \"Processing dataset ${DATADIR} ... \" \n",
    "cat \"${DATADIR}/train.csv\" | normalize_text > \"${DATADIR}/data.train\"\n",
    "cat \"${DATADIR}/test.csv\" | normalize_text > \"${DATADIR}/data.test\"\n",
    "echo \"done.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of tricks implementation\n",
    "\n",
    "The following dictionary class is reading some text corpus. The format is the same as for the official implementation of fastext, e.g.:\n",
    "\n",
    "    this is one sentence . here is a second sentence . __label__0\n",
    "    labels can be anywhere in the sentence . __label__5 one document per line .\n",
    "    \n",
    "To initialize a dictionary:\n",
    "\n",
    "```python\n",
    "    d = Dictionary()\n",
    "    file_stream = open(data_path, 'r')\n",
    "    d.read_from_file(file_stream)\n",
    "```\n",
    "\n",
    "Once the dictionary is initialized it is possible to read a file line per line with the same format as above:\n",
    "\n",
    "```python\n",
    "    file_stream = open(training_data_path, 'r')\n",
    "    words, labels, n_tokens = d.get_line(file_stream)\n",
    "```\n",
    "\n",
    "It will return a (words, labels, n_tokens) triplet: \n",
    "\n",
    "    [5, 869, 3, 7, 52, 67], [4], 6\n",
    "    \n",
    "The first list represents the idx of each token in the vocabulary list. The list contains only the unigrams. To add the n-grams and get a bag of tricks we can use `Dictionary.add_ngrams(words, n)`:\n",
    "\n",
    "```python\n",
    "    bag_of_tricks = d.add_ngrams(words, n=2)\n",
    "```\n",
    "\n",
    "We might get something like: \n",
    "\n",
    "    [5, 869, 3, 7, 52, 67, 500945, 105048, 780460, 100256, 407865]\n",
    "    \n",
    "You can see the method just appends the hashes of the bigrams in that words list. All the ids of the unigrams are in between [0, num_labels + num_words[, each id is associated to a unique unigram, this is not the case for the the n-grams. Each n-gram is associated to a hash, collisions may exist and one hash might be linked to multiple unigrams. The hashes are mapping n-grams to an integer in [num_labels + num_words, num_labels + num_words + buckets[ \n",
    "\n",
    "Having few collisions is fine, it's always the job of the learning algorithm to disentangle the factors a bit, collisions are making the learning task a bit more complicated but are allowing us to have a scalable input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def comp(x, y):\n",
    "    if x.is_label != y.is_label:\n",
    "        return -1 if x.is_label < y.is_label else 1\n",
    "    else: \n",
    "        return -1 if x.count > y.count else 1\n",
    "    \n",
    "class Dictionary():\n",
    "    \n",
    "    def __init__(self, MAX_VOCAB_SIZE=30000000, verbose=True, buckets=2000000, min_count=0):\n",
    "        self.min_count = min_count\n",
    "        self.MAX_VOCAB_SIZE = MAX_VOCAB_SIZE\n",
    "        self.EOS = '</s>'#end of sentence\n",
    "        self.word2int = [None] * self.MAX_VOCAB_SIZE #stores the position of each word in self.words\n",
    "        self.words = [] #stores the surface forms of each word\n",
    "        self.size = 0 #size of the vocabulary (labels + words)\n",
    "        self.nwords = 0 #number of unique words (labels excluded)\n",
    "        self.nlabels = 0 #number of unique labels\n",
    "        self.ntokens = 0 #total number of tokens\n",
    "        self.verbose = verbose\n",
    "        self.bucket = buckets #number of buckets for the ngrams, the higher the lesser collisions we have\n",
    "        \n",
    "    def find(self, word):\n",
    "        \"\"\"Use open addressing to get the hashing value of a given word.\"\"\"\n",
    "        h = self.hash_word(word) % self.MAX_VOCAB_SIZE\n",
    "        while self.word2int[h] is not None and self.words[self.word2int[h]].word != word: #open addressing\n",
    "            h = (h + 1) % self.MAX_VOCAB_SIZE\n",
    "        return h\n",
    "\n",
    "    def hash_word(self, s):\n",
    "        \"\"\"Compute a simple hashing value based on the characters of the string.\"\"\"\n",
    "        h = 2166136261\n",
    "        for i in range(len(s)):\n",
    "            h = h ^ ord(s[i])\n",
    "            h = h * 16777619\n",
    "        return h\n",
    "    \n",
    "    def add(self, word):\n",
    "        \"\"\"Add a word to the vocabulary if necessary or just increment its counter.\"\"\"\n",
    "        h = self.find(word)\n",
    "        self.ntokens += 1\n",
    "        if self.word2int[h] is None:\n",
    "            w_type = True if \"__label__\" in word else False\n",
    "            t = Token(word, 1, w_type)\n",
    "            self.words.append(t)\n",
    "            self.word2int[h] = self.size\n",
    "            self.size += 1\n",
    "        else:\n",
    "            self.words[self.word2int[h]].count += 1\n",
    "            \n",
    "    def read_from_file(self, file_stream):\n",
    "        \"\"\"Read a document, handle all the words in it, create the vocabulary\"\"\"\n",
    "        min_threshold = self.min_count #words of frequency <= to min_threshold are ignored\n",
    "        line = file_stream.readline()\n",
    "        while line:\n",
    "            line = re.split(' |\\n|\\t|\\v|\\f|\\r|\\0', line)\n",
    "            line.append(self.EOS)\n",
    "            for w in line:\n",
    "                if w == '': continue\n",
    "                self.add(w)\n",
    "                if self.ntokens % 10000000 == 0 and self.verbose:\n",
    "                    print(\"Read %d M words\" % (self.ntokens/1000000))\n",
    "                if self.size > 0.75 * self.MAX_VOCAB_SIZE:\n",
    "                    min_threshold += 1\n",
    "                    self.threshold(min_threshold)\n",
    "            line = file_stream.readline()\n",
    "        self.threshold(min_threshold)\n",
    "        if self.verbose:\n",
    "            print(\"\\rRead %d M words\" % (self.ntokens/1000000))\n",
    "            print(\"Number of unique words: %d\" % (self.nwords))\n",
    "            print(\"Number of labels: %d\" % (self.nlabels))\n",
    "        if self.size == 0:\n",
    "            print(\"Empty vocabulary. Try a smaller -minCount value.\")\n",
    "    \n",
    "    def threshold(self, min_value):\n",
    "        \"\"\"Remove all the words with a count less that min_value.\"\"\"\n",
    "        self.words.sort(key=cmp_to_key(comp))\n",
    "        self.words = [x for x in self.words if x.is_label or x.count > min_value]\n",
    "        self.size = 0\n",
    "        self.nwords = 0\n",
    "        self.nlabels = 0\n",
    "        self.word2int = [None] * self.MAX_VOCAB_SIZE\n",
    "        for t in self.words:\n",
    "            h = self.find(t.word)\n",
    "            self.word2int[h] = self.size\n",
    "            self.size += 1\n",
    "            if not t.is_label: self.nwords += 1\n",
    "            if t.is_label: self.nlabels += 1\n",
    "                \n",
    "    def add_ngrams(self, line_of_hs, n):\n",
    "        \"\"\"For a line of tokens, compute ann the n-grams, append them to the given list.\"\"\"\n",
    "        line_size = len(line_of_hs)\n",
    "        for i in range(line_size):\n",
    "            h = line_of_hs[i]\n",
    "            for j in range(i+1, i+n):\n",
    "                if j >= line_size: break\n",
    "                h = h * 116049371 + line_of_hs[j]\n",
    "                line_of_hs.append(self.nwords + (h % self.bucket))\n",
    "                \n",
    "    def get_id(self, word):\n",
    "        \"\"\"Return the id of the word in self.words list.\"\"\"\n",
    "        h = self.find(word)\n",
    "        return self.word2int[h]\n",
    "    \n",
    "    def get_type(self, word_id):\n",
    "        \"\"\"Return if the word is a label or not.\"\"\"\n",
    "        return self.words[word_id].is_label\n",
    "\n",
    "    def get_line(self, file_stream):\n",
    "        \"\"\"Return a list of word ids and a list of labels for one line.\"\"\"\n",
    "        n_tokens = 0\n",
    "        words = []\n",
    "        labels = []\n",
    "        line = file_stream.readline()\n",
    "        line = re.split(' |\\n|\\t|\\v|\\f|\\r|\\0', line)\n",
    "        line.append(self.EOS)\n",
    "        for w in line:\n",
    "            if w == '': continue\n",
    "            tid = self.get_id(w)\n",
    "            if tid is None: \n",
    "                continue\n",
    "            w_is_label = self.get_type(tid)\n",
    "            n_tokens += 1\n",
    "            if not w_is_label:\n",
    "                words.append(tid)\n",
    "            if w_is_label:\n",
    "                labels.append(tid - self.nwords) #the self.words list is sorted with labels at the end\n",
    "        return words, labels, n_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 10 M words\n",
      "Read 20 M words\n",
      "Read 30 M words\n",
      "Read 40 M words\n",
      "Read 50 M words\n",
      "Read 60 M words\n",
      "Read 70 M words\n",
      "Read 80 M words\n",
      "Read 90 M words\n",
      "Read 100 M words\n",
      "Read 105 M words\n",
      "Number of unique words: 185980\n",
      "Number of labels: 5\n"
     ]
    }
   ],
   "source": [
    "data_train_path = '/media/mat/ssdBackupMat/Datasets/sentiment/yelp_review_full_csv/data.train'\n",
    "\n",
    "min_count = 1\n",
    "buckets = 10000000\n",
    "\n",
    "dictionary = Dictionary(buckets=buckets, min_count=min_count)\n",
    "file_stream = open(data_train_path, 'r')\n",
    "dictionary.read_from_file(file_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Checking the labels count:\n",
    "\n",
    "    (__label__2, 130000), (__label__3, 130000), (__label__4, 130000), (__label__5, 130000), (__label__1, 130000)\n",
    "    \n",
    "\n",
    "Save / load the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dictionary_301016_full.pickle', 'wb') as f:\n",
    "    pickle.dump(dictionary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dictionary_301016_full.pickle', 'rb') as f:\n",
    "    dictionary = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext cbow tensorflow implementation\n",
    "\n",
    "### Utils\n",
    "\n",
    "I'm using minibatches instead of pure stochastic gradient descent like in the paper. This allows me to better approximate the gradient and learn faster (faster in terms of iterations). \n",
    "\n",
    "I am using sparse tensors to have compact minibatches, in tensorflow to create a sparse matrix we need:\n",
    "\n",
    "* sp_inputs_indices: contains the indices of non-zeros values in the sparse tensor\n",
    "* sp_inputs_ids_val: contains the values of the positions given by sp_inputs_indices\n",
    "* sp_inputs_shape: the shape of the sparse tensor: [BATCH_SIZE, max_num_entries]\n",
    "            \n",
    "My sparse tensor is representing something like:\n",
    "            \n",
    "    [[112, 5, 4, 3],\n",
    "     [1, 99, 85, 37, 47, 6],\n",
    "     [2, 9, 102]]\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(d, file_stream, batch_size, ngrams=2):\n",
    "    \"\"\"Return a mini-batch of size BATCH_SIZE: batch_X and batch_Y \n",
    "    containing respectively the words ids and labels.\n",
    "    \n",
    "    Arguments:\n",
    "        - d: dictionary to use\n",
    "        - file_stream: where to read the data from\n",
    "        - batch_size: the number of (document, label) pairs inside one batch\n",
    "        - ngrams: the ngram parameter, 2 for bigrams\n",
    "    \n",
    "    Returns:\n",
    "        - batch_X: a list of bag of tricks\n",
    "        \n",
    "                [[8,6,10,800,89846,123582,338745],\n",
    "                 [5,2,6,89456,1654984],\n",
    "                 [98,100,548,5,3,1,548998,102548,154789,132000,1459877]]\n",
    "                 \n",
    "        - batch_Y: a list of labels [1,4,0]\n",
    "    \n",
    "    \"\"\"\n",
    "    batch_X = []\n",
    "    batch_Y = []\n",
    "    tot_n_tokens = 0\n",
    "    for i in range(batch_size):\n",
    "        words, labels, n_tokens = d.get_line(file_stream)\n",
    "        tot_n_tokens += n_tokens\n",
    "        if len(labels) == 0: break\n",
    "        if ngrams>1: d.add_ngrams(words, ngrams)\n",
    "        batch_X.append(words)\n",
    "        batch_Y.append(labels[0])\n",
    "    return batch_X, batch_Y, tot_n_tokens\n",
    "\n",
    "def batch_to_sparse(batch_X):\n",
    "    \"\"\"Take a mini-batch as input and returns the components sp_inputs_indices, \n",
    "    sp_inputs_ids_val, and sp_inputs_shape, necessary to create a sparse \n",
    "    tensorflow tensor.\"\"\"\n",
    "    sp_inputs_indices = []\n",
    "    sp_inputs_ids_val = []\n",
    "    max_size = 0\n",
    "    for i in range(len(batch_X)):\n",
    "        sp_inputs_indices += [[i, j] for j in range(len(batch_X[i]))] #e.g. [[0,0],[0,1],[0,2],[1,0],[2,0],[2,1]]\n",
    "        max_size = max(max_size, len(batch_X[i]))\n",
    "        sp_inputs_ids_val += batch_X[i]\n",
    "    sp_inputs_shape = [len(batch_X), max_size]\n",
    "    return sp_inputs_indices, sp_inputs_ids_val, sp_inputs_shape\n",
    "\n",
    "def get_next_sparse_batch(d, file_stream, batch_size, ngrams=2):\n",
    "    \"\"\"Read from file_stream and generate sparse batches of size batch_size.\"\"\"\n",
    "    batch_X, batch_Y, n_tokens = get_batch(d, file_stream, batch_size, ngrams)\n",
    "    in_indices, in_ids_val, in_shape = batch_to_sparse(batch_X)\n",
    "    return in_indices, in_ids_val, in_shape, batch_Y, n_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_LABELS: 5\n",
      "INPUT_DIM: 10185980\n",
      "Tokens to process: 105094157\n"
     ]
    }
   ],
   "source": [
    "NUM_LABELS = dictionary.nlabels\n",
    "INPUT_DIM = dictionary.nwords + dictionary.bucket\n",
    "HIDDEN_SIZE = 10\n",
    "NGRAMS = 2 #bigrams\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 1e-2\n",
    "NUM_EPOCH = 1\n",
    "\n",
    "tokens_processed = 0.\n",
    "tokens_to_process = float(NUM_EPOCH * dictionary.ntokens)\n",
    "\n",
    "print(\"NUM_LABELS: %d\\nINPUT_DIM: %d\\nTokens to process: %d\" % (NUM_LABELS, INPUT_DIM, tokens_to_process))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, at each iteration I was reading from the file to create the new batch. This was very slow. To speed things up I used a tensorflow queue, that a thread is always filling with new batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import threading\n",
    "\n",
    "tf.reset_default_graph()#Reset the graph essential to use with jupyter else variable conflicts\n",
    "\n",
    "class QueueCtrl(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"The init links the input tensors with the enqueue operation that will be used\n",
    "        to fill the queue. \n",
    "        \"\"\"\n",
    "        self.sp_inputs_indices = tf.placeholder(tf.int64)\n",
    "        self.sp_inputs_ids_val = tf.placeholder(tf.int64)\n",
    "        self.sp_inputs_shape = tf.placeholder(tf.int64)\n",
    "        self.inputs_num_tokens = tf.placeholder(tf.int64)\n",
    "        \n",
    "        self.labels = tf.placeholder(tf.int64, shape=(None), name='labels')\n",
    "        \n",
    "        self.queue = tf.FIFOQueue(dtypes=[tf.int64, tf.int64, tf.int64, tf.int64, tf.int64],\n",
    "                                           capacity=500)\n",
    "\n",
    "        self.enqueue_op = self.queue.enqueue([self.sp_inputs_indices,\n",
    "                                              self.sp_inputs_ids_val,\n",
    "                                              self.sp_inputs_shape,\n",
    "                                              self.labels,\n",
    "                                              self.inputs_num_tokens])\n",
    "\n",
    "    def get_batch_from_queue(self):\n",
    "        \"\"\"Return one batch\n",
    "        \"\"\"\n",
    "        return self.queue.dequeue()\n",
    "\n",
    "    def thread_main(self, sess, coord, data_path):\n",
    "        \"\"\"Function nexecuted by the thread. Loop over the data, add the minibatches to the queue. \n",
    "        Stops when the coordinator says so.\n",
    "        \"\"\"\n",
    "        print(\"Starting publisher thread: %d\" % (threading.get_ident()))\n",
    "        train_fs = open(data_path, 'r')\n",
    "        while not coord.should_stop():\n",
    "            in_indices, in_ids_val, in_shape, batch_Y, n_tokens = get_next_sparse_batch(dictionary, train_fs, \n",
    "                                                                                        BATCH_SIZE, NGRAMS)\n",
    "            if not batch_Y: #EOF\n",
    "                train_fs = open(data_path, 'r')\n",
    "                in_indices, in_ids_val, in_shape, batch_Y, n_tokens = get_next_sparse_batch(dictionary, train_fs, \n",
    "                                                                                            BATCH_SIZE, NGRAMS)\n",
    "            sess.run(self.enqueue_op, feed_dict={self.sp_inputs_indices:in_indices, \n",
    "                                                 self.sp_inputs_ids_val:in_ids_val,\n",
    "                                                 self.sp_inputs_shape:in_shape,\n",
    "                                                 self.labels:batch_Y,\n",
    "                                                 self.inputs_num_tokens:n_tokens}) #append batch to the queue\n",
    "\n",
    "    def start_thread(self, sess, coord, data_path):\n",
    "        \"\"\"Start the thread\"\"\"\n",
    "        t = threading.Thread(target=self.thread_main, args=(sess, coord, data_path))\n",
    "        t.daemon = True\n",
    "        t.start()\n",
    "        return [t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell finishes to build the graph, it:\n",
    "* reads from the queue\n",
    "* uses a sparse tensor to get the average of all the n-grams embeddings for each document in the minibatch.\n",
    "* computes the logits and softmax probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queue_ctrl = QueueCtrl()\n",
    "in_indices, in_ids_val, in_shape, batch_Y, n_tokens = queue_ctrl.get_batch_from_queue()\n",
    "\n",
    "sp_inputs_ids = tf.SparseTensor(in_indices, in_ids_val, in_shape) \n",
    "\n",
    "embedding_matrix = tf.get_variable(\"embeddings\", [INPUT_DIM, HIDDEN_SIZE], tf.float32,\n",
    "                                   initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "hidden_vectors = tf.nn.embedding_lookup_sparse(embedding_matrix, sp_inputs_ids, None, \n",
    "                                               name=\"averaged_embeddings\",\n",
    "                                               combiner=\"mean\") #average the embeddings for each input\n",
    "\n",
    "context_matrix = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, NUM_LABELS], \n",
    "                                                 mean=0.0, stddev=2./(NUM_LABELS+HIDDEN_SIZE), \n",
    "                                                 dtype=tf.float32, \n",
    "                                                 name='context_matrix'))\n",
    "\n",
    "logits = tf.matmul(hidden_vectors, context_matrix)\n",
    "\n",
    "lr = tf.Variable(LEARNING_RATE, trainable=False)\n",
    "\n",
    "#Getting the probabilities and accuracy (Used at inference time):\n",
    "probabilities = tf.nn.softmax(logits, name=\"softmax\")\n",
    "correct_predictions = tf.equal(tf.argmax(probabilities,1), batch_Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "\n",
    "#Defining the error metric used and loss (Used at training time):\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, batch_Y, name=\"cross_entropy\")\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "#Initialization op:\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "\n",
    "If we want to train on GPU, checks if it is already used by someone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov  2 15:43:30 2016       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX TIT...  Off  | 0000:01:00.0      On |                  N/A |\n",
      "| 22%   49C    P2    75W / 250W |   1118MiB / 12203MiB |      1%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID  Type  Process name                               Usage      |\n",
      "|=============================================================================|\n",
      "|    0      1152    G   /usr/lib/xorg/Xorg                             471MiB |\n",
      "|    0      2032    G   cinnamon                                       123MiB |\n",
      "|    0      2550    G   ...s-passed-by-fd --v8-snapshot-passed-by-fd   136MiB |\n",
      "|    0      3329    C   /usr/bin/python3                               324MiB |\n",
      "|    0     12296    G   ...s-passed-by-fd --v8-snapshot-passed-by-fd    57MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now that our graph is ready we can start the training procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_model(model_name):\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    data_train_path = '/media/mat/ssdBackupMat/Datasets/sentiment/yelp_review_full_csv/data.train'\n",
    "    start_time = time.time()\n",
    "    PRINT_EVERY = 200\n",
    "\n",
    "    sess = tf.Session()\n",
    "\n",
    "    sess.run(init)\n",
    "    coord = tf.train.Coordinator()\n",
    "    tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    my_thread = queue_ctrl.start_thread(sess, coord, data_train_path) #start the publisher thread\n",
    "    iteration, progress = 0, 0.\n",
    "    tokens_processed = 0.\n",
    "    \n",
    "    print(\"\\nTraining model: %s\" % model_name)\n",
    "\n",
    "    while progress < 100:\n",
    "        _, loss_, n_tokens_ = sess.run([train_step, loss, n_tokens])\n",
    "        tokens_processed += n_tokens_\n",
    "        progress = tokens_processed / tokens_to_process *100.\n",
    "        if iteration % PRINT_EVERY == 0:\n",
    "            print(\"Iter: %d, %.2f%% done, Minibatch loss: %.4f, Elements in queue: %d\" \n",
    "                  % (iteration, progress, loss_, sess.run(queue_ctrl.queue.size())))\n",
    "        iteration += 1\n",
    "\n",
    "    coord.request_stop()\n",
    "\n",
    "    print(\"Done. Exec time: %.2f minutes.\" % ((time.time() - start_time) / 60.))\n",
    "    save_path = saver.save(sess, model_name)\n",
    "\n",
    "    coord.join(my_thread, stop_grace_period_secs=10)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have statistical significance and be sure that the network did not found a good local minimum by chance, we create multiple models initialized differently and compute the mean accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting publisher thread: 139863026284288\n",
      "\n",
      "Training model: ./model_cbow_yelp_f_2910160.ckpt\n",
      "Iter: 0, 0.04% done, Minibatch loss: 1.6094, Elements in queue: 1\n",
      "Iter: 200, 7.91% done, Minibatch loss: 1.0139, Elements in queue: 0\n",
      "Iter: 400, 15.72% done, Minibatch loss: 0.9048, Elements in queue: 0\n",
      "Iter: 600, 23.53% done, Minibatch loss: 0.8986, Elements in queue: 0\n",
      "Iter: 800, 31.38% done, Minibatch loss: 0.9170, Elements in queue: 0\n",
      "Iter: 1000, 39.21% done, Minibatch loss: 0.8480, Elements in queue: 0\n",
      "Iter: 1200, 47.05% done, Minibatch loss: 0.8765, Elements in queue: 0\n",
      "Iter: 1400, 54.96% done, Minibatch loss: 0.8457, Elements in queue: 0\n",
      "Iter: 1600, 62.73% done, Minibatch loss: 0.8192, Elements in queue: 0\n",
      "Iter: 1800, 70.63% done, Minibatch loss: 0.9096, Elements in queue: 0\n",
      "Iter: 2000, 78.50% done, Minibatch loss: 0.8324, Elements in queue: 0\n",
      "Iter: 2200, 86.38% done, Minibatch loss: 0.7777, Elements in queue: 0\n",
      "Iter: 2400, 94.24% done, Minibatch loss: 0.9104, Elements in queue: 0\n",
      "Done. Exec time: 13.01 minutes.\n",
      "Starting publisher thread: 139863026284288\n",
      "\n",
      "Training model: ./model_cbow_yelp_f_2910161.ckpt\n",
      "Iter: 0, 0.04% done, Minibatch loss: 1.6094, Elements in queue: 0\n",
      "Iter: 200, 7.91% done, Minibatch loss: 1.0098, Elements in queue: 0\n",
      "Iter: 400, 15.72% done, Minibatch loss: 0.9009, Elements in queue: 0\n",
      "Iter: 600, 23.53% done, Minibatch loss: 0.8935, Elements in queue: 0\n",
      "Iter: 800, 31.38% done, Minibatch loss: 0.9226, Elements in queue: 0\n",
      "Iter: 1000, 39.21% done, Minibatch loss: 0.8425, Elements in queue: 0\n",
      "Iter: 1200, 47.05% done, Minibatch loss: 0.8759, Elements in queue: 0\n",
      "Iter: 1400, 54.96% done, Minibatch loss: 0.8394, Elements in queue: 0\n",
      "Iter: 1600, 62.73% done, Minibatch loss: 0.8153, Elements in queue: 0\n",
      "Iter: 1800, 70.63% done, Minibatch loss: 0.9041, Elements in queue: 0\n",
      "Iter: 2000, 78.50% done, Minibatch loss: 0.8273, Elements in queue: 0\n",
      "Iter: 2200, 86.38% done, Minibatch loss: 0.7772, Elements in queue: 0\n",
      "Iter: 2400, 94.24% done, Minibatch loss: 0.9099, Elements in queue: 0\n",
      "Done. Exec time: 13.43 minutes.\n",
      "Starting publisher thread: 139863026284288\n",
      "\n",
      "Training model: ./model_cbow_yelp_f_2910162.ckpt\n",
      "Iter: 0, 0.04% done, Minibatch loss: 1.6094, Elements in queue: 0\n",
      "Iter: 200, 7.91% done, Minibatch loss: 1.0113, Elements in queue: 0\n",
      "Iter: 400, 15.72% done, Minibatch loss: 0.9021, Elements in queue: 0\n",
      "Iter: 600, 23.53% done, Minibatch loss: 0.8945, Elements in queue: 0\n",
      "Iter: 800, 31.38% done, Minibatch loss: 0.9204, Elements in queue: 0\n",
      "Iter: 1000, 39.21% done, Minibatch loss: 0.8424, Elements in queue: 0\n",
      "Iter: 1200, 47.05% done, Minibatch loss: 0.8768, Elements in queue: 0\n",
      "Iter: 1400, 54.96% done, Minibatch loss: 0.8397, Elements in queue: 0\n",
      "Iter: 1600, 62.73% done, Minibatch loss: 0.8151, Elements in queue: 0\n",
      "Iter: 1800, 70.63% done, Minibatch loss: 0.9034, Elements in queue: 0\n",
      "Iter: 2000, 78.50% done, Minibatch loss: 0.8266, Elements in queue: 0\n",
      "Iter: 2200, 86.38% done, Minibatch loss: 0.7779, Elements in queue: 0\n",
      "Iter: 2400, 94.24% done, Minibatch loss: 0.9093, Elements in queue: 0\n",
      "Done. Exec time: 13.29 minutes.\n",
      "Starting publisher thread: 139863026284288\n",
      "\n",
      "Training model: ./model_cbow_yelp_f_2910163.ckpt\n",
      "Iter: 0, 0.04% done, Minibatch loss: 1.6094, Elements in queue: 0\n",
      "Iter: 200, 7.91% done, Minibatch loss: 1.0102, Elements in queue: 0\n",
      "Iter: 400, 15.72% done, Minibatch loss: 0.9021, Elements in queue: 0\n",
      "Iter: 600, 23.53% done, Minibatch loss: 0.8948, Elements in queue: 0\n",
      "Iter: 800, 31.38% done, Minibatch loss: 0.9211, Elements in queue: 0\n",
      "Iter: 1000, 39.21% done, Minibatch loss: 0.8450, Elements in queue: 0\n",
      "Iter: 1200, 47.05% done, Minibatch loss: 0.8778, Elements in queue: 0\n",
      "Iter: 1400, 54.96% done, Minibatch loss: 0.8425, Elements in queue: 0\n",
      "Iter: 1600, 62.73% done, Minibatch loss: 0.8180, Elements in queue: 0\n",
      "Iter: 1800, 70.63% done, Minibatch loss: 0.9090, Elements in queue: 0\n",
      "Iter: 2000, 78.50% done, Minibatch loss: 0.8297, Elements in queue: 0\n",
      "Iter: 2200, 86.38% done, Minibatch loss: 0.7791, Elements in queue: 0\n",
      "Iter: 2400, 94.24% done, Minibatch loss: 0.9091, Elements in queue: 0\n",
      "Done. Exec time: 12.53 minutes.\n",
      "Starting publisher thread: 139863026284288\n",
      "\n",
      "Training model: ./model_cbow_yelp_f_2910164.ckpt\n",
      "Iter: 0, 0.04% done, Minibatch loss: 1.6094, Elements in queue: 0\n",
      "Iter: 200, 7.91% done, Minibatch loss: 1.0107, Elements in queue: 0\n",
      "Iter: 400, 15.72% done, Minibatch loss: 0.9000, Elements in queue: 0\n",
      "Iter: 600, 23.53% done, Minibatch loss: 0.8927, Elements in queue: 0\n",
      "Iter: 800, 31.38% done, Minibatch loss: 0.9211, Elements in queue: 0\n",
      "Iter: 1000, 39.21% done, Minibatch loss: 0.8442, Elements in queue: 0\n",
      "Iter: 1200, 47.05% done, Minibatch loss: 0.8789, Elements in queue: 0\n",
      "Iter: 1400, 54.96% done, Minibatch loss: 0.8405, Elements in queue: 0\n",
      "Iter: 1600, 62.73% done, Minibatch loss: 0.8163, Elements in queue: 0\n",
      "Iter: 1800, 70.63% done, Minibatch loss: 0.9038, Elements in queue: 0\n",
      "Iter: 2000, 78.50% done, Minibatch loss: 0.8276, Elements in queue: 0\n",
      "Iter: 2200, 86.38% done, Minibatch loss: 0.7777, Elements in queue: 0\n",
      "Iter: 2400, 94.24% done, Minibatch loss: 0.9083, Elements in queue: 0\n",
      "Done. Exec time: 12.45 minutes.\n",
      "Starting publisher thread: 139863026284288\n",
      "\n",
      "Training model: ./model_cbow_yelp_f_2910165.ckpt\n",
      "Iter: 0, 0.04% done, Minibatch loss: 1.6094, Elements in queue: 0\n",
      "Iter: 200, 7.91% done, Minibatch loss: 1.0149, Elements in queue: 0\n",
      "Iter: 400, 15.72% done, Minibatch loss: 0.9075, Elements in queue: 0\n",
      "Iter: 600, 23.53% done, Minibatch loss: 0.8990, Elements in queue: 0\n",
      "Iter: 800, 31.38% done, Minibatch loss: 0.9170, Elements in queue: 0\n",
      "Iter: 1000, 39.21% done, Minibatch loss: 0.8445, Elements in queue: 0\n",
      "Iter: 1200, 47.05% done, Minibatch loss: 0.8777, Elements in queue: 0\n",
      "Iter: 1400, 54.96% done, Minibatch loss: 0.8459, Elements in queue: 0\n",
      "Iter: 1600, 62.73% done, Minibatch loss: 0.8171, Elements in queue: 0\n",
      "Iter: 1800, 70.63% done, Minibatch loss: 0.9034, Elements in queue: 0\n",
      "Iter: 2000, 78.50% done, Minibatch loss: 0.8299, Elements in queue: 0\n",
      "Iter: 2200, 86.38% done, Minibatch loss: 0.7798, Elements in queue: 0\n",
      "Iter: 2400, 94.24% done, Minibatch loss: 0.9080, Elements in queue: 0\n",
      "Done. Exec time: 12.53 minutes.\n",
      "Starting publisher thread: 139863026284288\n",
      "\n",
      "Training model: ./model_cbow_yelp_f_2910166.ckpt\n",
      "Iter: 0, 0.04% done, Minibatch loss: 1.6094, Elements in queue: 0\n",
      "Iter: 200, 7.91% done, Minibatch loss: 1.0117, Elements in queue: 0\n",
      "Iter: 400, 15.72% done, Minibatch loss: 0.9038, Elements in queue: 0\n",
      "Iter: 600, 23.53% done, Minibatch loss: 0.8995, Elements in queue: 0\n",
      "Iter: 800, 31.38% done, Minibatch loss: 0.9203, Elements in queue: 0\n",
      "Iter: 1000, 39.21% done, Minibatch loss: 0.8490, Elements in queue: 0\n",
      "Iter: 1200, 47.05% done, Minibatch loss: 0.8808, Elements in queue: 0\n",
      "Iter: 1400, 54.96% done, Minibatch loss: 0.8459, Elements in queue: 0\n",
      "Iter: 1600, 62.73% done, Minibatch loss: 0.8207, Elements in queue: 0\n",
      "Iter: 1800, 70.63% done, Minibatch loss: 0.9087, Elements in queue: 0\n",
      "Iter: 2000, 78.50% done, Minibatch loss: 0.8256, Elements in queue: 0\n",
      "Iter: 2200, 86.38% done, Minibatch loss: 0.7859, Elements in queue: 0\n",
      "Iter: 2400, 94.24% done, Minibatch loss: 0.9092, Elements in queue: 0\n",
      "Done. Exec time: 12.20 minutes.\n",
      "Starting publisher thread: 139863026284288\n",
      "\n",
      "Training model: ./model_cbow_yelp_f_2910167.ckpt\n",
      "Iter: 0, 0.04% done, Minibatch loss: 1.6094, Elements in queue: 0\n",
      "Iter: 200, 7.91% done, Minibatch loss: 1.0135, Elements in queue: 0\n",
      "Iter: 400, 15.72% done, Minibatch loss: 0.9106, Elements in queue: 0\n",
      "Iter: 600, 23.53% done, Minibatch loss: 0.8993, Elements in queue: 0\n",
      "Iter: 800, 31.38% done, Minibatch loss: 0.9185, Elements in queue: 0\n",
      "Iter: 1000, 39.21% done, Minibatch loss: 0.8465, Elements in queue: 0\n",
      "Iter: 1200, 47.05% done, Minibatch loss: 0.8793, Elements in queue: 0\n",
      "Iter: 1400, 54.96% done, Minibatch loss: 0.8380, Elements in queue: 0\n",
      "Iter: 1600, 62.73% done, Minibatch loss: 0.8163, Elements in queue: 0\n",
      "Iter: 1800, 70.63% done, Minibatch loss: 0.9009, Elements in queue: 0\n",
      "Iter: 2000, 78.50% done, Minibatch loss: 0.8260, Elements in queue: 0\n",
      "Iter: 2200, 86.38% done, Minibatch loss: 0.7792, Elements in queue: 0\n",
      "Iter: 2400, 94.24% done, Minibatch loss: 0.9085, Elements in queue: 0\n",
      "Done. Exec time: 12.75 minutes.\n",
      "Starting publisher thread: 139863026284288\n",
      "\n",
      "Training model: ./model_cbow_yelp_f_2910168.ckpt\n",
      "Iter: 0, 0.04% done, Minibatch loss: 1.6094, Elements in queue: 0\n",
      "Iter: 200, 7.91% done, Minibatch loss: 1.0252, Elements in queue: 0\n",
      "Iter: 400, 15.72% done, Minibatch loss: 0.9097, Elements in queue: 0\n",
      "Iter: 600, 23.53% done, Minibatch loss: 0.9022, Elements in queue: 0\n",
      "Iter: 800, 31.38% done, Minibatch loss: 0.9208, Elements in queue: 0\n",
      "Iter: 1000, 39.21% done, Minibatch loss: 0.8469, Elements in queue: 0\n",
      "Iter: 1200, 47.05% done, Minibatch loss: 0.8807, Elements in queue: 0\n",
      "Iter: 1400, 54.96% done, Minibatch loss: 0.8468, Elements in queue: 0\n",
      "Iter: 1600, 62.73% done, Minibatch loss: 0.8172, Elements in queue: 0\n",
      "Iter: 1800, 70.63% done, Minibatch loss: 0.9054, Elements in queue: 0\n",
      "Iter: 2000, 78.50% done, Minibatch loss: 0.8314, Elements in queue: 0\n",
      "Iter: 2200, 86.38% done, Minibatch loss: 0.7825, Elements in queue: 0\n",
      "Iter: 2400, 94.24% done, Minibatch loss: 0.9076, Elements in queue: 0\n",
      "Done. Exec time: 12.09 minutes.\n",
      "Starting publisher thread: 139863026284288\n",
      "\n",
      "Training model: ./model_cbow_yelp_f_2910169.ckpt\n",
      "Iter: 0, 0.04% done, Minibatch loss: 1.6094, Elements in queue: 0\n",
      "Iter: 200, 7.91% done, Minibatch loss: 1.0134, Elements in queue: 0\n",
      "Iter: 400, 15.72% done, Minibatch loss: 0.9067, Elements in queue: 0\n",
      "Iter: 600, 23.53% done, Minibatch loss: 0.8971, Elements in queue: 0\n",
      "Iter: 800, 31.38% done, Minibatch loss: 0.9135, Elements in queue: 0\n",
      "Iter: 1000, 39.21% done, Minibatch loss: 0.8445, Elements in queue: 0\n",
      "Iter: 1200, 47.05% done, Minibatch loss: 0.8765, Elements in queue: 0\n",
      "Iter: 1400, 54.96% done, Minibatch loss: 0.8393, Elements in queue: 0\n",
      "Iter: 1600, 62.73% done, Minibatch loss: 0.8143, Elements in queue: 0\n",
      "Iter: 1800, 70.63% done, Minibatch loss: 0.9031, Elements in queue: 0\n",
      "Iter: 2000, 78.50% done, Minibatch loss: 0.8297, Elements in queue: 0\n",
      "Iter: 2200, 86.38% done, Minibatch loss: 0.7790, Elements in queue: 0\n",
      "Iter: 2400, 94.24% done, Minibatch loss: 0.9091, Elements in queue: 0\n",
      "Done. Exec time: 12.33 minutes.\n"
     ]
    }
   ],
   "source": [
    "model_base_name = './model_cbow_yelp_f_291016'\n",
    "\n",
    "model_names = []\n",
    "\n",
    "for i in range(10):\n",
    "    name = model_base_name + str(i) + '.ckpt'\n",
    "    model_names.append(name)\n",
    "    train_model(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Testing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test_path = '/media/mat/ssdBackupMat/Datasets/sentiment/yelp_review_full_csv/data.test'\n",
    "\n",
    "def compute_accuracy(label_probas):\n",
    "    \"\"\"Takes as input a list of tuples [(true_label, [p1, p2, ...]), ...] it\n",
    "    outputs the accuracy.\n",
    "    \"\"\"\n",
    "    good_guesses = 0.\n",
    "    total =0.\n",
    "    for label, probas in label_probas:\n",
    "        if label == np.argmax(probas):\n",
    "            good_guesses += 1.\n",
    "        total += 1.\n",
    "    return good_guesses / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we infer the probabilities for the reviews of the test set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 64.25%, std: 0.04\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "\n",
    "for model_path in model_names:\n",
    "    \n",
    "    predictions = []\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        saver.restore(sess, model_path)\n",
    "\n",
    "        test_fs = open(data_test_path, 'r')\n",
    "        in_indices_, in_ids_val_, in_shape_, batch_Y_, n_tokens = get_next_sparse_batch(dictionary, test_fs, \n",
    "                                                                                        512, NGRAMS)\n",
    "        while batch_Y_:\n",
    "\n",
    "            p = sess.run(probabilities, feed_dict={\n",
    "              in_indices: in_indices_,  \n",
    "              in_shape: in_shape_, \n",
    "              in_ids_val: in_ids_val_,\n",
    "              batch_Y: batch_Y_\n",
    "            })\n",
    "            predictions += list(zip(batch_Y_, p))\n",
    "            in_indices_, in_ids_val_, in_shape_, batch_Y_, n_tokens = get_next_sparse_batch(dictionary, test_fs, \n",
    "                                                                                            512, NGRAMS)\n",
    "    accuracy.append(compute_accuracy(predictions) * 100.)\n",
    "\n",
    "print(\"Accuracy of the model: %.2f%%, std: %.2f\" % (np.mean(accuracy), np.std(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In the paper they got 63.9% accuracy on the same dataset, they also report a state of the art of 64.7%. From my experiments, early stopping should be used if possible. Using minibatches and adam optimizer I was able to converge much faster (1 epoch) than with pure stochastic gradient descent with learning rate decay (5 epochs in the paper). Using 2 or 3 epochs in my case yielded overfitting and a lower accuracy. \n",
    "\n",
    "The code is a bit slow with about 15 minutes for one epoch (vary depending on the batch size). One of the main bottleneck is streaming the corpus and creating the batches. I tried to speed it up using a queue, but queues are a mess in tensorflow if your data is sparse since you cannot use `enqueue_many` and `dequeue_many` and publishers and subsribers are fighting for the access to the queue yielding a longer training time. Using one publisher helped a bit though. Seeing the queue always hints us that some improvements might still be possible, by preprocessing things a bit more for example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ordinal regression\n",
    "\n",
    "Mistaking a 5 stars review with a 4 star reviews and mistaking a 5 stars review with a 1 star review is not the same! Instead of doing classification, we can better evaluate the error by doing ordinal regression. We just need to change the output layer and the loss of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fastext for word embeddings\n",
    "\n",
    "The original paper can be found here: https://arxiv.org/pdf/1607.04606.pdf\n",
    "\n",
    "## Bag of tricks and preprocessing\n",
    "\n",
    "### Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Token():\n",
    "    \"\"\"A simple class to represent the tokens.\"\"\"\n",
    "    def __init__(self, word, count, subwords):\n",
    "        self.word = word\n",
    "        self.count = count\n",
    "        self.subwords = [] #will contain all the ngrams for this word\n",
    "    def __str__(self):\n",
    "        return '('+self.word+', '+str(self.count)+')'\n",
    "    def __repr__(self):\n",
    "        return '('+self.word+', '+str(self.count)+')'\n",
    "    \n",
    "def cmp_to_key(mycmp):\n",
    "    \"\"\"Convert a cmp= function into a key= function\"\"\"\n",
    "    class K:\n",
    "        def __init__(self, obj, *args):\n",
    "            self.obj = obj\n",
    "        def __lt__(self, other):\n",
    "            return mycmp(self.obj, other.obj) < 0\n",
    "        def __gt__(self, other):\n",
    "            return mycmp(self.obj, other.obj) > 0\n",
    "        def __eq__(self, other):\n",
    "            return mycmp(self.obj, other.obj) == 0\n",
    "        def __le__(self, other):\n",
    "            return mycmp(self.obj, other.obj) <= 0\n",
    "        def __ge__(self, other):\n",
    "            return mycmp(self.obj, other.obj) >= 0\n",
    "        def __ne__(self, other):\n",
    "            return mycmp(self.obj, other.obj) != 0\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of tricks: Character ngrams\n",
    "\n",
    "The following dictionary class is reading some text corpus. I am using an already tokenized and pre-processed version of wikipedia. Here is a small part of it, each line represents the context of one sentence:\n",
    "\n",
    "    the _data_ flag is defined in signed by former president _person_ the flag design is similar to that of the flag of the vice president of the united states \n",
    "    duriron company the duriron company is an industrial component manufacturer of such products as automatic control valves and actuators pumps sealing systems filtration equipment pipes and fittings \n",
    "    the company was incorporated in _data_ as the duriron casting company in dayton ohio by _person_ and _person_ \n",
    "    the company derives its name from a high silicon cast iron alloy duriron which rapidly became an industry standard for handling extremely corrosive materials \n",
    "    duriron denitrating towers were in high demand during the first world war for safe handling of hot mixtures of nitric and sulfuric acids during the manufacturing of explosives \n",
    "    as a result of the war the company expanded tenfold increasing its work force from _number_ to _number_ \n",
    "    during _data_ and _number_ duriron faced a major readjustment period since denitrating towers used in the war effort were no longer in demand \n",
    "    the readjustment included the development of a line of more off the shelf duriron pumps and valves for use in the broad chemical process industries\n",
    "    \n",
    "To initialize a dictionary:\n",
    "\n",
    "```python\n",
    "    d = Dictionary()\n",
    "    file_stream = open(data_path, 'r')\n",
    "    d.read_from_file(file_stream)\n",
    "```\n",
    "\n",
    "Once the dictionary is initialized it is possible to read a file line per line:\n",
    "\n",
    "```python\n",
    "    file_stream = open(training_data_path, 'r')\n",
    "    words, labels, n_tokens = d.get_line(file_stream)\n",
    "```\n",
    "`Dictionary.get_line(file_stream)` applies some subsampling to prevent frequent words to appear in all possible contexts. Words are discarded with probability $p$ depending on their frequency $f$ in the corpus:\n",
    "\n",
    "$$p = 1 - \\sqrt{\\frac{t}{f}} - \\frac{t}{f}$$\n",
    "\n",
    "Where $t$ is an hyperparameter controlling the subsampling rate, the lower the more subsampling. \n",
    "    \n",
    "Just like for the previously studied CBOW setup, the index of each word is mapped to [0, vocabulary_size[, and the hashing trick is applied to associate a hash to each subword-ngram squashing them with collisions in [vocabulary_size, vocabulary_size+buckets["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def comp(x, y):\n",
    "    return -1 if x.count > y.count else 1\n",
    "    \n",
    "class Dictionary():\n",
    "    \n",
    "    def __init__(self, MAX_VOCAB_SIZE=30000000, verbose=True, buckets=2000000, min_count=0):\n",
    "        self.min_count = min_count\n",
    "        self.MAX_VOCAB_SIZE = MAX_VOCAB_SIZE\n",
    "        self.MAX_LINE_SIZE = 1024\n",
    "        self.EOS = '</s>'#end of sentence\n",
    "        self.BOW = '<'#token added before the word to consider prefixes\n",
    "        self.EOW = '>'#token added after the word to consider suffixes\n",
    "        self.word2int = [None] * self.MAX_VOCAB_SIZE #stores the position of each word in self.words\n",
    "        self.words = [] #stores the surface forms of each word\n",
    "        self.size = 0 #size of the vocabulary (labels + words)\n",
    "        self.nwords = 0 #number of unique words (labels excluded)\n",
    "        self.ntokens = 0 #total number of tokens\n",
    "        self.verbose = verbose\n",
    "        self.pkeep = []\n",
    "        self.sampling_threshold = 1e-4\n",
    "        self.min_ngram_size = 3\n",
    "        self.max_ngram_size = 6\n",
    "        self.bucket = buckets #number of buckets for the ngrams, the higher the lesser collisions we have\n",
    "        \n",
    "    def find(self, word):\n",
    "        \"\"\"Use open addressing to get the hashing value of a given word.\"\"\"\n",
    "        h = self.hash_word(word) % self.MAX_VOCAB_SIZE\n",
    "        while self.word2int[h] is not None and self.words[self.word2int[h]].word != word: #open addressing\n",
    "            h = (h + 1) % self.MAX_VOCAB_SIZE\n",
    "        return h\n",
    "\n",
    "    def hash_word(self, s):\n",
    "        \"\"\"Compute a simple hashing value based on the characters of the string.\"\"\"\n",
    "        h = 2166136261\n",
    "        for i in range(len(s)):\n",
    "            h = h ^ ord(s[i])\n",
    "            h = h * 16777619\n",
    "        return h\n",
    "    \n",
    "    def init_ngrams(self):\n",
    "        \"\"\"Add the prefix and suffixe token to each word, add its idx to the list of subwords, \n",
    "        compute all the subwords\"\"\"\n",
    "        for i in range(self.size):\n",
    "            word = self.BOW + self.words[i].word + self.EOW\n",
    "            self.words[i].subwords.append(i)\n",
    "            self.compute_ngrams(word, i)\n",
    "            \n",
    "    def compute_ngrams(self, word, idx):\n",
    "        \"\"\"Compute all the sub-ngrams of size self.between min_ngram_size and \n",
    "        self.max_ngram_size, and give them an id in the range \n",
    "        [self.nwords, self.nwords+self.buckets]\"\"\"\n",
    "        for i in range(len(word)):\n",
    "            ngram = \"\"\n",
    "            for j in range(i, i+self.max_ngram_size):\n",
    "                if j >= len(word) or len(ngram) >= self.max_ngram_size:\n",
    "                    break\n",
    "                ngram += word[j]\n",
    "                if self.min_ngram_size <= len(ngram):\n",
    "                    h = self.hash_word(ngram) % self.bucket\n",
    "                    self.words[idx].subwords.append(self.nwords + h)\n",
    "    \n",
    "    def add(self, word):\n",
    "        \"\"\"Add a word to the vocabulary if necessary or just increment its counter.\"\"\"\n",
    "        h = self.find(word)\n",
    "        self.ntokens += 1\n",
    "        if self.word2int[h] is None:\n",
    "            t = Token(word, 1, [])\n",
    "            self.words.append(t)\n",
    "            self.word2int[h] = self.size\n",
    "            self.size += 1\n",
    "        else:\n",
    "            self.words[self.word2int[h]].count += 1\n",
    "            \n",
    "    def read_from_file(self, file_stream):\n",
    "        \"\"\"Read a document, handle all the words in it, create the vocabulary\"\"\"\n",
    "        min_threshold = self.min_count #words of frequency <= to min_threshold are ignored\n",
    "        line = file_stream.readline()\n",
    "        while line:\n",
    "            line = re.split(' |\\n|\\t|\\v|\\f|\\r|\\0', line)\n",
    "            for w in line:\n",
    "                if w == '': continue\n",
    "                self.add(w)\n",
    "                if self.ntokens % 100000000 == 0 and self.verbose:\n",
    "                    print(\"Read %d M words\" % (self.ntokens/1000000))\n",
    "                if self.size > 0.75 * self.MAX_VOCAB_SIZE:\n",
    "                    min_threshold += 1\n",
    "                    self.threshold(min_threshold)\n",
    "            line = file_stream.readline()\n",
    "        self.threshold(min_threshold)\n",
    "        self.init_table_discard()\n",
    "        self.init_ngrams()\n",
    "        if self.verbose:\n",
    "            print(\"\\rRead %d M words\" % (self.ntokens/1000000))\n",
    "            print(\"Number of unique words: %d\" % (self.nwords))\n",
    "        if self.size == 0:\n",
    "            print(\"Empty vocabulary. Try a smaller -minCount value.\")\n",
    "            \n",
    "    def init_table_discard(self):\n",
    "        \"\"\"\"\"\"\n",
    "        for i in range(self.size):\n",
    "            f = float(self.words[i].count) / float(self.ntokens)\n",
    "            self.pkeep.append(np.sqrt(self.sampling_threshold / f) + self.sampling_threshold / f)\n",
    "\n",
    "    def discard(self, idx, rnd):\n",
    "        \"\"\"Keep the word with a probability of self.pkeep[idx].\"\"\"\n",
    "        return rnd > self.pkeep[idx]\n",
    "    \n",
    "    def threshold(self, min_value):\n",
    "        \"\"\"Remove all the words with a count less that min_value.\"\"\"\n",
    "        self.words.sort(key=cmp_to_key(comp))\n",
    "        self.words = [x for x in self.words if x.count > min_value]\n",
    "        self.size = 0\n",
    "        self.nwords = 0\n",
    "        self.word2int = [None] * self.MAX_VOCAB_SIZE\n",
    "        for t in self.words:\n",
    "            h = self.find(t.word)\n",
    "            self.word2int[h] = self.size\n",
    "            self.size += 1\n",
    "            self.nwords += 1\n",
    "                \n",
    "    def get_id(self, word):\n",
    "        \"\"\"Return the id of the word in self.words list.\"\"\"\n",
    "        h = self.find(word)\n",
    "        return self.word2int[h]\n",
    "\n",
    "    def get_line(self, file_stream):\n",
    "        \"\"\"Return a list of word ids and a list of labels for one line.\n",
    "        Returns (-1,-1) if EOF.\"\"\"\n",
    "        n_tokens = 0\n",
    "        words = []\n",
    "        line = file_stream.readline()\n",
    "        if line == '':\n",
    "            return -1, -1\n",
    "        line = re.split(' |\\n|\\t|\\v|\\f|\\r|\\0', line)\n",
    "        for w in line:\n",
    "            if w == '': continue\n",
    "            tid = self.get_id(w)\n",
    "            if tid is None: \n",
    "                continue\n",
    "            n_tokens += 1\n",
    "            if not self.discard(tid, random.random()): \n",
    "                words.append(tid)\n",
    "            if len(words) > self.MAX_LINE_SIZE:\n",
    "                break\n",
    "        return words, n_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word we generate all the ngrams, for the word logistic, the tokens `<` and `>` are added to differentiate prefixes and suffixes from the rest:\n",
    "\n",
    "    <lo, <log, <logi, <logis, log, logi, logis, logist, ogi, ogis, ogist, ogisti, gis, gist, gisti, gistic, ist, isti, istic, istic>, sti, stic, stic>, tic, tic>, ic>\n",
    "    \n",
    "The surface form itself 'logistic' is also added to the set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 10 M words\n",
      "Read 20 M words\n",
      "Read 30 M words\n",
      "Read 40 M words\n",
      "Read 50 M words\n",
      "Read 60 M words\n",
      "Read 70 M words\n",
      "Read 80 M words\n",
      "Read 90 M words\n",
      "Read 100 M words\n",
      "Read 110 M words\n",
      "Read 120 M words\n",
      "Read 130 M words\n",
      "Read 140 M words\n",
      "Read 150 M words\n",
      "Read 160 M words\n",
      "Read 170 M words\n",
      "Read 180 M words\n",
      "Read 190 M words\n",
      "Read 200 M words\n",
      "Read 210 M words\n",
      "Read 220 M words\n",
      "Read 230 M words\n",
      "Read 240 M words\n",
      "Read 250 M words\n",
      "Read 260 M words\n",
      "Read 270 M words\n",
      "Read 280 M words\n",
      "Read 290 M words\n",
      "Read 300 M words\n",
      "Read 310 M words\n",
      "Read 320 M words\n",
      "Read 330 M words\n",
      "Read 340 M words\n",
      "Read 350 M words\n",
      "Read 360 M words\n",
      "Read 370 M words\n",
      "Read 380 M words\n",
      "Read 390 M words\n",
      "Read 400 M words\n",
      "Read 410 M words\n",
      "Read 420 M words\n",
      "Read 430 M words\n",
      "Read 440 M words\n",
      "Read 450 M words\n",
      "Read 460 M words\n",
      "Read 470 M words\n",
      "Read 480 M words\n",
      "Read 490 M words\n",
      "Read 500 M words\n",
      "Read 510 M words\n",
      "Read 520 M words\n",
      "Read 530 M words\n",
      "Read 540 M words\n",
      "Read 550 M words\n",
      "Read 560 M words\n",
      "Read 570 M words\n",
      "Read 580 M words\n",
      "Read 590 M words\n",
      "Read 600 M words\n",
      "Read 610 M words\n",
      "Read 620 M words\n",
      "Read 630 M words\n",
      "Read 640 M words\n",
      "Read 650 M words\n",
      "Read 660 M words\n",
      "Read 670 M words\n",
      "Read 680 M words\n",
      "Read 690 M words\n",
      "Read 700 M words\n",
      "Read 710 M words\n",
      "Read 720 M words\n",
      "Read 730 M words\n",
      "Read 740 M words\n",
      "Read 750 M words\n",
      "Read 760 M words\n",
      "Read 770 M words\n",
      "Read 780 M words\n",
      "Read 790 M words\n",
      "Read 800 M words\n",
      "Read 810 M words\n",
      "Read 820 M words\n",
      "Read 830 M words\n",
      "Read 840 M words\n",
      "Read 850 M words\n",
      "Read 860 M words\n",
      "Read 870 M words\n",
      "Read 880 M words\n",
      "Read 890 M words\n",
      "Read 900 M words\n",
      "Read 910 M words\n",
      "Read 920 M words\n",
      "Read 930 M words\n",
      "Read 940 M words\n",
      "Read 950 M words\n",
      "Read 960 M words\n",
      "Read 970 M words\n",
      "Read 980 M words\n",
      "Read 990 M words\n",
      "Read 1000 M words\n",
      "Read 1010 M words\n",
      "Read 1020 M words\n",
      "Read 1030 M words\n",
      "Read 1040 M words\n",
      "Read 1050 M words\n",
      "Read 1060 M words\n",
      "Read 1070 M words\n",
      "Read 1080 M words\n",
      "Read 1090 M words\n",
      "Read 1100 M words\n",
      "Read 1110 M words\n",
      "Read 1120 M words\n",
      "Read 1130 M words\n",
      "Read 1140 M words\n",
      "Read 1150 M words\n",
      "Read 1160 M words\n",
      "Read 1170 M words\n",
      "Read 1180 M words\n",
      "Read 1190 M words\n",
      "Read 1200 M words\n",
      "Read 1210 M words\n",
      "Read 1220 M words\n",
      "Read 1230 M words\n",
      "Read 1240 M words\n",
      "Read 1250 M words\n",
      "Read 1260 M words\n",
      "Read 1270 M words\n",
      "Read 1280 M words\n",
      "Read 1290 M words\n",
      "Read 1300 M words\n",
      "Read 1310 M words\n",
      "Read 1320 M words\n",
      "Read 1330 M words\n",
      "Read 1340 M words\n",
      "Read 1350 M words\n",
      "Read 1360 M words\n",
      "Read 1370 M words\n",
      "Read 1380 M words\n",
      "Read 1390 M words\n",
      "Read 1400 M words\n",
      "Read 1410 M words\n",
      "Read 1420 M words\n",
      "Read 1430 M words\n",
      "Read 1440 M words\n",
      "Read 1450 M words\n",
      "Read 1460 M words\n",
      "Read 1470 M words\n",
      "Read 1480 M words\n",
      "Read 1490 M words\n",
      "Read 1500 M words\n",
      "Read 1510 M words\n",
      "Read 1520 M words\n",
      "Read 1530 M words\n",
      "Read 1540 M words\n",
      "Read 1550 M words\n",
      "Read 1560 M words\n",
      "Read 1570 M words\n",
      "Read 1573 M words\n",
      "Number of unique words: 1093071\n"
     ]
    }
   ],
   "source": [
    "fs = open('./wiki_corpus/wiki_full_word2vec.txt', 'r')\n",
    "\n",
    "wiki_dict = Dictionary(min_count=5, buckets=2000000)\n",
    "wiki_dict.read_from_file(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dictionary_wiki_301016_full.pickle', 'wb') as f:\n",
    "    pickle.dump(wiki_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dictionary_wiki_301016_full.pickle', 'rb') as f:\n",
    "    wiki_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext skipgram tensorflow implementation\n",
    "\n",
    "The begining of the graph is almost the same as for the cbow one, but we modify the output to train using negative sampling. Before the implementation was cbow because we were using the context (words in document) to predict the class, now we are using the class (current word) to predict the context (words around the current words).\n",
    "\n",
    "For the input layer we sum the character ngrams representations instead of averaging them as for the cbow.\n",
    "\n",
    "### Utils\n",
    "\n",
    "With the following code we can generate pairs of (context / word) as done in the official word2vec package. Careful, the file generated is huuuuge (362GB). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def get_pairs_from_line(line, d, window_size):\n",
    "    \"\"\"Use the window size to generate a batch of (ngrams, target_word) pairs.\"\"\"\n",
    "    batch_X = []\n",
    "    batch_Y = []\n",
    "    for i in range(len(line)):\n",
    "        b = random.randint(1, window_size)\n",
    "        ngrams = d.words[line[i]].subwords #the representation of the input\n",
    "        for j in range(-b,b+1):\n",
    "            if j != 0 and 0 <= i+j < len(line):\n",
    "                batch_X.append(ngrams)\n",
    "                batch_Y.append(line[i+j])\n",
    "    return batch_X, batch_Y, len(batch_Y)\n",
    "\n",
    "def generate_pairs_file(file_stream, d, window_size, out_stream):\n",
    "    \"\"\"Iterate over the entire corpus, generate all the pairs and save them in a file:\n",
    "    \n",
    "                [1, 25, 48, 7]|6 -> On the left: all the ids of the ngrams for the word in the middle of the window\n",
    "                [156, 5, 9]|58   -> The label on the right is a word in the context\n",
    "                ...\n",
    "                \n",
    "    \"\"\"\n",
    "    line, num_t = d.get_line(file_stream)\n",
    "    while num_t != -1:\n",
    "        batch_X, batch_Y, size = get_pairs_from_line(line, d, window_size)\n",
    "        for i in range(size):\n",
    "            ngrams = batch_X[i]\n",
    "            target = batch_Y[i]\n",
    "            out_stream.write(str(ngrams) + '|' + str(target) + '\\n')\n",
    "        line, num_t = d.get_line(file_stream)\n",
    "        \n",
    "def batch_to_sparse(batch_X):\n",
    "    \"\"\"Take a mini-batch as input and return the components sp_inputs_indices, sp_inputs_ids_val, and \n",
    "    sp_inputs_shape, necessary to create a sparse tensorflow tensor.\"\"\"\n",
    "    sp_inputs_indices = []\n",
    "    sp_inputs_ids_val = []\n",
    "    max_size = 0\n",
    "    for i in range(len(batch_X)):\n",
    "        sp_inputs_indices += [[i, j] for j in range(len(batch_X[i]))] #e.g. [[0,0],[0,1],[0,2],[1,0],[2,0],[2,1]]\n",
    "        max_size = max(max_size, len(batch_X[i]))\n",
    "        sp_inputs_ids_val += batch_X[i]\n",
    "    sp_inputs_shape = [len(batch_X), max_size]\n",
    "    return sp_inputs_indices, sp_inputs_ids_val, sp_inputs_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell generate the file of pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 5\n",
    "\n",
    "with open('./wiki_corpus/wiki_full_word2vec.txt', 'r') as in_stream:\n",
    "    with open('/media/mat/ssdBackupMat/Datasets/Wikipedia/pairs_fasttext/pairs.txt', 'w') as out_stream:\n",
    "        generate_pairs_file(in_stream, wiki_dict, WINDOW_SIZE, out_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000000 /media/mat/ssdBackupMat/Datasets/Wikipedia/pairs_fasttext/wiki_60M_pairs.shuf\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wc -l /media/mat/ssdBackupMat/Datasets/Wikipedia/pairs_fasttext/wiki_60M_pairs.shuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_sparse_batch(file_stream, batch_size):\n",
    "    \"\"\"Read the file of pairs and generate a minibatch\n",
    "    \"\"\"\n",
    "    batch_X = []\n",
    "    batch_Y = []\n",
    "    while len(batch_X) < batch_size:\n",
    "        line = file_stream.readline()\n",
    "        if line == '': #EOF\n",
    "            break\n",
    "        try:\n",
    "            X, Y = line.split('|')\n",
    "            X = eval(X)\n",
    "            Y = eval(Y)\n",
    "            batch_X.append(X)\n",
    "            batch_Y.append([Y])\n",
    "        except:\n",
    "            continue\n",
    "    in_indices, in_ids_val, in_shape = batch_to_sparse(batch_X)\n",
    "    return in_indices, in_ids_val, in_shape, batch_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first I used a queue here as well not to wait for the data to be fetched. However I found that they were leaking in my RAM that was getting filled slowly. Apparently they [already got this issue](https://github.com/tensorflow/tensorflow/issues/2942#event-699911869) in the past... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE: 1093071\n",
      "INPUT_SIZE: 3093071\n",
      "Pairs to process: 30000000\n"
     ]
    }
   ],
   "source": [
    "NUM_NEG_SAMPLES = 9\n",
    "VOCAB_SIZE = wiki_dict.nwords\n",
    "INPUT_SIZE = VOCAB_SIZE + wiki_dict.bucket\n",
    "HIDDEN_SIZE = 80\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = .02 \n",
    "PORTION_TO_PROCESS = 1. #Will process 100% of the training set\n",
    "NUM_EPOCH = 1\n",
    "PAIRS_FILE_PATH = '/media/mat/ssdBackupMat/Datasets/Wikipedia/pairs_fasttext/wiki_30M_pairs.shuf'\n",
    "\n",
    "pairs_processed = 0.\n",
    "pairs_to_process = 30000000\n",
    "\n",
    "print(\"VOCAB_SIZE: %d\\nINPUT_SIZE: %d\\nPairs to process: %d\" % (VOCAB_SIZE, INPUT_SIZE, pairs_to_process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "sp_inputs_indices = tf.placeholder(tf.int64)\n",
    "sp_inputs_ids_val = tf.placeholder(tf.int64)\n",
    "sp_inputs_shape = tf.placeholder(tf.int64)\n",
    "\n",
    "labels = tf.placeholder(tf.int64)\n",
    "\n",
    "sp_inputs_ids = tf.SparseTensor(sp_inputs_indices, sp_inputs_ids_val, sp_inputs_shape) \n",
    "\n",
    "embedding_matrix = tf.get_variable(\"embeddings\", [INPUT_SIZE, HIDDEN_SIZE], tf.float32,\n",
    "                                   initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "hidden_vectors = tf.nn.embedding_lookup_sparse(embedding_matrix, sp_inputs_ids, None, \n",
    "                                               name=\"sum_of_embeddings\",\n",
    "                                               combiner=\"sum\") #sum the embeddings for each input\n",
    "\n",
    "context_matrix = tf.get_variable(\"context_matrix\", [VOCAB_SIZE, HIDDEN_SIZE], tf.float32,\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "out_biases = tf.Variable(tf.zeros([VOCAB_SIZE]), trainable=False)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(context_matrix, out_biases, hidden_vectors, labels,\n",
    "                                     NUM_NEG_SAMPLES, VOCAB_SIZE))\n",
    "\n",
    "lr = tf.placeholder(tf.float32, shape=[])\n",
    "train_step = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov  3 08:55:08 2016       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX TIT...  Off  | 0000:01:00.0      On |                  N/A |\n",
      "| 22%   48C    P2    74W / 250W |  11674MiB / 12203MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID  Type  Process name                               Usage      |\n",
      "|=============================================================================|\n",
      "|    0       506    C   /usr/bin/python3                             10427MiB |\n",
      "|    0      1152    G   /usr/lib/xorg/Xorg                             592MiB |\n",
      "|    0      2032    G   cinnamon                                       105MiB |\n",
      "|    0      2550    G   ...s-passed-by-fd --v8-snapshot-passed-by-fd   159MiB |\n",
      "|    0      3329    C   /usr/bin/python3                               324MiB |\n",
      "|    0     12296    G   ...s-passed-by-fd --v8-snapshot-passed-by-fd    57MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "PRINT_EVERY = 5000\n",
    "DECAY_LR_EVERY = 5000 #every 50 iterations\n",
    "\n",
    "lr_ = LEARNING_RATE\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "    iteration, progress = 0, 0.\n",
    "    \n",
    "    train_fs = open(PAIRS_FILE_PATH, 'r')\n",
    "    in_indices, in_ids_val, in_shape, batch_Y = get_next_sparse_batch(train_fs, BATCH_SIZE)\n",
    "\n",
    "    while progress < PORTION_TO_PROCESS*100. and batch_Y: \n",
    "\n",
    "        _, loss_ = sess.run([train_step, loss], feed_dict={\n",
    "                sp_inputs_indices:in_indices, \n",
    "                sp_inputs_ids_val:in_ids_val,\n",
    "                sp_inputs_shape:in_shape,\n",
    "                labels:batch_Y,\n",
    "                lr: lr_\n",
    "        }) \n",
    "        pairs_processed += float(len(batch_Y))\n",
    "        progress = pairs_processed / pairs_to_process *100.\n",
    "        if iteration % DECAY_LR_EVERY == 0:\n",
    "            lr_ = max(1e-6, (1. - pairs_processed / (PORTION_TO_PROCESS * pairs_to_process))) * LEARNING_RATE\n",
    "        if iteration % PRINT_EVERY == 0:\n",
    "            print(\"Iter: %d, %.2f%% done, Minibatch loss: %.4f, lr: %.4f\" % (iteration, progress, loss_, lr_))\n",
    "        iteration += 1\n",
    "\n",
    "        in_indices, in_ids_val, in_shape, batch_Y = get_next_sparse_batch(train_fs, BATCH_SIZE)\n",
    "\n",
    "    print(\"Done. Exec time: %.2f hours.\" % ((time.time() - start_time) / 3600.))\n",
    "    save_path = saver.save(sess, \"./model_skipgram_wiki_291016.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've found out that using stochastic gradient descent was very important to enable convergence. I couldn't converge with batches of 256 or 512. Batch size of 16 or 8 are ok.   \n",
    "\n",
    "Since I cannot use big batches I cannot really justify using my GPU and the training is very slow. For this reason I trained on only less than 5% (about 130M pairs) of wikipedia, which is not much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Playing with the embeddings\n",
    "\n",
    "Now we can getthe embedding representation for all of the words in our vocabulary. We also get a matrix of random weights to be able to validate the learning.\n",
    "\n",
    "#### With the standard model (taking the sum of all subwords embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = \"./model_skipgram_wiki_291016.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the embedding matrix: (1093071, 80)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver.restore(sess, save_path)\n",
    "    \n",
    "    all_ngrams = [w.subwords for w in wiki_dict.words]\n",
    "    \n",
    "    in_indices_, in_ids_val_, in_shape_ = batch_to_sparse(all_ngrams)\n",
    "\n",
    "    embedding_matrix = sess.run(hidden_vectors, feed_dict={\n",
    "          sp_inputs_indices: in_indices_,  \n",
    "          sp_inputs_shape: in_shape_, \n",
    "          sp_inputs_ids_val: in_ids_val_\n",
    "    })\n",
    "\n",
    "print(\"Shape of the embedding matrix:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the embedding matrix: (1093071, 80)\n"
     ]
    }
   ],
   "source": [
    "random_embedding_matrix = None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    all_ngrams = [w.subwords for w in wiki_dict.words]\n",
    "    \n",
    "    in_indices_, in_ids_val_, in_shape_ = batch_to_sparse(all_ngrams)\n",
    "\n",
    "    random_embedding_matrix = sess.run(hidden_vectors, feed_dict={\n",
    "          sp_inputs_indices: in_indices_,  \n",
    "          sp_inputs_shape: in_shape_, \n",
    "          sp_inputs_ids_val: in_ids_val_\n",
    "    })\n",
    "\n",
    "print(\"Shape of the embedding matrix:\", random_embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_array(mat):\n",
    "    for idx in range(mat.shape[0]):\n",
    "        mat[idx] /= np.linalg.norm(mat[idx])\n",
    "        \n",
    "def cosine_sim(mat, vec):\n",
    "    \"\"\"Assumes normalized arrays\"\"\"\n",
    "    cosine_sims = np.inner(vec, mat)\n",
    "    return np.argsort(cosine_sims)[::-1], cosine_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalize_array(embedding_matrix)\n",
    "normalize_array(random_embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We can do some neirest neighbors queries for some simple words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query word: switzerland\n",
      "\n",
      "Top similarities with the trained model:\n",
      "1.00 -- switzerland\n",
      "0.99 -- switzerlands\n",
      "0.99 -- austria/switzerland\n",
      "0.98 -- switzeland\n",
      "0.98 -- kritzerland\n",
      "0.98 -- italy\n",
      "0.98 -- austria\n",
      "0.98 -- sweden\n",
      "0.98 -- netherlands\n",
      "0.98 -- netherland\n",
      "0.97 -- austria-este\n",
      "0.97 -- switserland\n",
      "0.97 -- luxemburg\n",
      "0.97 -- ufo-sweden\n",
      "0.97 -- denmark\n",
      "\n",
      "Only displaying words not containing the query word:\n",
      "0.98 -- switzeland\n",
      "0.98 -- kritzerland\n",
      "0.98 -- italy\n",
      "0.98 -- austria\n",
      "0.98 -- sweden\n",
      "0.98 -- netherlands\n",
      "0.98 -- netherland\n",
      "0.97 -- austria-este\n",
      "0.97 -- switserland\n",
      "0.97 -- luxemburg\n",
      "0.97 -- ufo-sweden\n",
      "0.97 -- denmark\n",
      "0.97 -- finland\n",
      "0.97 -- finland-sweden\n",
      "0.97 -- luxembourg\n",
      "\n",
      "Top similarities with the random initialized matrix:\n",
      "0.53 -- can-tv\n",
      "0.50 -- osterburken\n",
      "0.49 -- 431,000\n",
      "0.49 -- osgi\n",
      "0.48 -- 169yb\n",
      "0.48 -- machans\n",
      "0.46 -- wrsy\n",
      "0.46 -- hans-g\n",
      "0.46 -- pre-conventional\n",
      "0.45 -- b-300\n"
     ]
    }
   ],
   "source": [
    "word2idx = {w: idx for idx, w in enumerate([w.word for w in wiki_dict.words])}\n",
    "\n",
    "query_word = 'switzerland'\n",
    "query_word_idx = word2idx[query_word]\n",
    "query_word_embedding = embedding_matrix[query_word_idx]\n",
    "\n",
    "print(\"Query word: %s\" % query_word)\n",
    "print(\"\\nTop similarities with the trained model:\")\n",
    "top_sim, sims = cosine_sim(embedding_matrix, query_word_embedding)\n",
    "for i in top_sim[:15]:\n",
    "    print(\"%.2f -- %s\" % (sims[i], wiki_dict.words[i].word))\n",
    "    \n",
    "print(\"\\nOnly displaying words not containing the query word:\")\n",
    "i, inc = 0, 0\n",
    "while i < 15:\n",
    "    idx = top_sim[inc]\n",
    "    word = wiki_dict.words[idx].word\n",
    "    if query_word not in word:\n",
    "        i += 1\n",
    "        print(\"%.2f -- %s\" % (sims[idx], wiki_dict.words[idx].word))\n",
    "    inc += 1\n",
    "    \n",
    "print(\"\\nTop similarities with the random initialized matrix:\")\n",
    "top_sim, sims = cosine_sim(random_embedding_matrix, query_word_embedding)\n",
    "for i in top_sim[:10]:\n",
    "    print(\"%.2f -- %s\" % (sims[i], wiki_dict.words[i].word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query word: science\n",
      "\n",
      "Top similarities with the trained model:\n",
      "1.00 -- science\n",
      "0.99 -- sciencenow\n",
      "0.99 -- geoscience\n",
      "0.99 -- science/math\n",
      "0.99 -- sciencenet\n",
      "0.99 -- geo-science\n",
      "0.99 -- math/science\n",
      "0.99 -- science.the\n",
      "0.99 -- docscience\n",
      "0.99 -- math-science\n",
      "0.99 -- science/arts\n",
      "0.99 -- cyberscience\n",
      "0.99 -- science.he\n",
      "0.99 -- bio-science\n",
      "0.99 -- science.\n",
      "\n",
      "Only displaying words not containing the query word:\n",
      "0.98 -- sciencies\n",
      "0.98 -- sciencie\n",
      "0.98 -- sciencoj\n",
      "0.98 -- scienca\n",
      "0.98 -- scientex\n",
      "0.97 -- scientomogy\n",
      "0.97 -- scieneer\n",
      "0.97 -- scient\n",
      "0.97 -- tecnology\n",
      "0.97 -- technnology\n",
      "0.97 -- scienter\n",
      "0.97 -- hi-teknology\n",
      "0.97 -- cology\n",
      "0.97 -- knology\n",
      "0.97 -- doxology\n"
     ]
    }
   ],
   "source": [
    "word2idx = {w: idx for idx, w in enumerate([w.word for w in wiki_dict.words])}\n",
    "\n",
    "query_word = 'science'\n",
    "query_word_idx = word2idx[query_word]\n",
    "query_word_embedding = embedding_matrix[query_word_idx]\n",
    "\n",
    "print(\"Query word: %s\" % query_word)\n",
    "print(\"\\nTop similarities with the trained model:\")\n",
    "top_sim, sims = cosine_sim(embedding_matrix, query_word_embedding)\n",
    "for i in top_sim[:15]:\n",
    "    print(\"%.2f -- %s\" % (sims[i], wiki_dict.words[i].word))\n",
    "    \n",
    "print(\"\\nOnly displaying words not containing the query word:\")\n",
    "i, inc = 0, 0\n",
    "while i < 15:\n",
    "    idx = top_sim[inc]\n",
    "    word = wiki_dict.words[idx].word\n",
    "    if query_word not in word:\n",
    "        i += 1\n",
    "        print(\"%.2f -- %s\" % (sims[idx], wiki_dict.words[idx].word))\n",
    "    inc += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for not so simple words like `english-born`. For this word the paper show the top two most similar:\n",
    "* british-born\n",
    "* polish-born"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query word: english-born\n",
      "\n",
      "Top similarities with the trained model:\n",
      "1.00 -- english-born\n",
      "0.98 -- welsh-born\n",
      "0.98 -- irish-born\n",
      "0.98 -- essex-born\n",
      "0.97 -- devon-born\n",
      "0.97 -- scots-born\n",
      "0.97 -- uk-born\n",
      "0.97 -- dutch-born\n",
      "0.97 -- derry-born\n",
      "0.97 -- ao-born\n",
      "0.97 -- nz-born\n",
      "0.97 -- tongan-born\n",
      "0.97 -- new-born\n",
      "0.97 -- dublin-born\n",
      "0.97 -- flemish-born\n",
      "0.97 -- manx-born\n",
      "0.97 -- bermudian-born\n",
      "0.97 -- twyborn\n",
      "0.97 -- barbadian-born\n",
      "0.97 -- dundee-born\n",
      "0.97 -- galician-born\n",
      "0.97 -- danish-born\n",
      "0.97 -- dorset-born\n",
      "0.97 -- maine-born\n",
      "0.97 -- burmese-born\n"
     ]
    }
   ],
   "source": [
    "word2idx = {w: idx for idx, w in enumerate([w.word for w in wiki_dict.words])}\n",
    "\n",
    "query_word = 'english-born'\n",
    "query_word_idx = word2idx[query_word]\n",
    "query_word_embedding = embedding_matrix[query_word_idx]\n",
    "\n",
    "print(\"Query word: %s\" % query_word)\n",
    "print(\"\\nTop similarities with the trained model:\")\n",
    "top_sim, sims = cosine_sim(embedding_matrix, query_word_embedding)\n",
    "for i in top_sim[:25]:\n",
    "    print(\"%.2f -- %s\" % (sims[i], wiki_dict.words[i].word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query word: micromanaging\n",
      "\n",
      "Top similarities with the trained model:\n",
      "1.00 -- micromanaging\n",
      "0.99 -- micro-managing\n",
      "0.98 -- micromanager\n",
      "0.98 -- micromanage\n",
      "0.98 -- ceo/managing\n",
      "0.97 -- micromanagement\n",
      "0.97 -- microbanking\n",
      "0.97 -- micromanagers\n",
      "0.97 -- microlending\n",
      "0.97 -- geomarketing\n",
      "0.97 -- micromarketing\n",
      "0.97 -- managing\n",
      "0.97 -- micro-pnt\n",
      "0.97 -- self-managing\n",
      "0.97 -- micro-ct\n",
      "0.97 -- micro-imaging\n",
      "0.97 -- micro-financing\n",
      "0.97 -- micro-dosing\n",
      "0.97 -- micronics\n",
      "0.97 -- microblogging\n",
      "0.97 -- microbsd\n",
      "0.97 -- micro-ice\n",
      "0.97 -- micro-sd\n",
      "0.97 -- macromarketing\n",
      "0.97 -- co-managing\n"
     ]
    }
   ],
   "source": [
    "word2idx = {w: idx for idx, w in enumerate([w.word for w in wiki_dict.words])}\n",
    "\n",
    "query_word = 'micromanaging'\n",
    "query_word_idx = word2idx[query_word]\n",
    "query_word_embedding = embedding_matrix[query_word_idx]\n",
    "\n",
    "print(\"Query word: %s\" % query_word)\n",
    "print(\"\\nTop similarities with the trained model:\")\n",
    "top_sim, sims = cosine_sim(embedding_matrix, query_word_embedding)\n",
    "for i in top_sim[:25]:\n",
    "    print(\"%.2f -- %s\" % (sims[i], wiki_dict.words[i].word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "If we don't consider the n-grams and only retrieve the vectors for each unigram (no sum of embeddings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the embedding matrix: (1093071, 80)\n"
     ]
    }
   ],
   "source": [
    "unigrams_embedding_matrix = None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver.restore(sess, save_path)\n",
    "    \n",
    "    all_ngrams = [[w.subwords[0]] for w in wiki_dict.words]\n",
    "    \n",
    "    in_indices_, in_ids_val_, in_shape_ = batch_to_sparse(all_ngrams)\n",
    "\n",
    "    unigrams_embedding_matrix = sess.run(hidden_vectors, feed_dict={\n",
    "          sp_inputs_indices: in_indices_,  \n",
    "          sp_inputs_shape: in_shape_, \n",
    "          sp_inputs_ids_val: in_ids_val_\n",
    "    })\n",
    "\n",
    "print(\"Shape of the embedding matrix:\", unigrams_embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalize_array(unigrams_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query word: science\n",
      "\n",
      "Top similarities with the trained model:\n",
      "1.00 -- science\n",
      "0.72 -- electives\n",
      "0.72 -- cyberpunk\n",
      "0.72 -- concise\n",
      "0.71 -- gamut\n",
      "0.71 -- gadgets\n",
      "0.70 -- offbeat\n",
      "0.69 -- communication\n",
      "0.69 -- fiction\n",
      "0.68 -- digitized\n",
      "0.68 -- puzzles\n",
      "0.67 -- recipes\n",
      "0.66 -- kata\n",
      "0.66 -- re-published\n",
      "0.65 -- collage\n",
      "0.65 -- cooking\n",
      "0.65 -- possibilities\n",
      "0.65 -- delved\n",
      "0.65 -- richness\n",
      "0.65 -- math\n",
      "0.64 -- concisely\n",
      "0.64 -- speculative\n",
      "0.64 -- introductory\n",
      "0.64 -- quilt\n",
      "0.64 -- typography\n"
     ]
    }
   ],
   "source": [
    "word2idx = {w: idx for idx, w in enumerate([w.word for w in wiki_dict.words])}\n",
    "\n",
    "query_word = 'science'\n",
    "query_word_idx = word2idx[query_word]\n",
    "query_word_embedding = unigrams_embedding_matrix[query_word_idx]\n",
    "\n",
    "print(\"Query word: %s\" % query_word)\n",
    "print(\"\\nTop similarities with the trained model:\")\n",
    "top_sim, sims = cosine_sim(unigrams_embedding_matrix, query_word_embedding)\n",
    "for i in top_sim[:25]:\n",
    "    print(\"%.2f -- %s\" % (sims[i], wiki_dict.words[i].word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end the embeddings trained are not that good, but they've been trained only on about 2% of the wikipedia corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others\n",
    "\n",
    "to get the table of contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
